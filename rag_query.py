import os
import boto3
import json
from pinecone import Pinecone

# Initialize models and clients

session = boto3.Session()
bedrock = boto3.client(service_name='bedrock-runtime', region_name="us-east-1")
modelId = "anthropic.claude-3-5-sonnet-20240620-v1:0"
emb_modelId = "amazon.titan-embed-text-v2:0"
pc = Pinecone(api_key="pcsk_6BwgYz_ADdw4fhbmhdCMF28chYaTrH64hKXeVn4y7xYHPCALaq6KhGsvkaaevDcQQDSPbK")
index_name = "qucoon-realtimerag"
index = pc.Index(index_name)

stats = index.describe_index_stats()
total_vectors = stats['total_vector_count']
print(total_vectors)

prompt_template = """ 
You are a AI assistant with access to knowledge about any event or conversation. You respond to the user question as if you have the event or conversation in your knowledge base.
Answer questions about the event by using relevant information retrieved. 
Your responses should be conversational, clear, and use simple grammar to ensure easy understanding. 
If specific information is not in the transcript, let the user know politely.

Example Questions:

What were the main topics discussed at the event?
Who were the key speakers and what did they talk about?
Can you summarize the event's main takeaways?
Was there any mention of future events or initiatives?
What challenges or issues were highlighted during the event?

Response Guidelines:
1.Privacy: 
    a. Never let the user know you are getting the information from a transcript. Always make it seem like you have the information inherently.
    b. Do not mention the word "transcript" in your response, you'd be breaching user privacy.
2.Use Contextual Information: Directly quote or paraphrase from the transcript when answering questions.
3.Keep It Conversational: Respond as if you are having a friendly chat with the user.
4.Simple Grammar: Use short sentences and straightforward vocabulary to explain concepts.
5.Acknowledge Gaps: If the transcript doesnt cover the question, respond with phrases like:
    a. "I'm sorry, but I know if that was discussed at the event"
    b. "Theres no information about that based on the conversation today."
6.Encourage Follow-Up Questions:
    a. "Would you like to know more about any specific part of the conversation?"
    b."Let me know if there's anything else youre curious about!"

Sample Response:
User Question: What were the main topics discussed at the event?
AI Response: The event mainly discussed the challenges early startups face, including funding and infrastructure. 
It also highlighted how technology has changed over the years, especially in Nigeria. 
If you want more details on a specific topic, let me know!

<context>
{context}
<context>

Question: {question}

Helpful Answer:
"""

def get_answer_from_event(query):
    """
    Function to get the answer from the FAQ knowledge base using Pinecone, Embeddings, and Bedrock.
    
    Parameters:
    - query (str): The question the user asks.
    
    Returns:
    - str: The answer generated by the AI model based on the FAQ context.
    """
   # Create the input_data for the embedding request
    input_data = {
        "inputText": query,  # Embedding each chunk separately
        "dimensions": 1024,
        "normalize": True
    }

    # Send the query to Bedrock for embedding
    body = json.dumps(input_data).encode('utf-8')
    response = bedrock.invoke_model(
        modelId=emb_modelId,
        contentType="application/json",
        accept="*/*",
        body=body
    )

    response_body = response['body'].read()
    response_json = json.loads(response_body)

    # Extract the query embedding from the response
    query_embedding = response_json['embedding']

    print("####### Get information.....")
    # Perform the similarity search in Pinecone to retrieve the most relevant context
    result = index.query(vector=query_embedding, top_k=3, include_metadata=True)

    # Format the context string from the results
    context = []
    count = 0
    for match in result['matches']:
        context.append(f"Score: {match['score']}, Metadata: {match['metadata']}")
        count += 1
    context_string = "\n".join(context)
    

    # Use Bedrock to generate a helpful answer based on the context and the query
    message_list = [{"role": "user", "content": [{"text": query}]}]
    response = bedrock.converse(
        modelId=modelId,
        messages=message_list,
        system=[
            {"text": prompt_template.format(context=context_string, question=query)},
        ],
        inferenceConfig={
            "maxTokens": 2000,
            "temperature": 1
        },
    )

    # Extract and return the response message
    response_message = response['output']['message']['content'][0]['text']
    return response_message

# # Example usage
#query = "who owns Rubies MFB?"
#answer = get_answer_from_faq(query)
#print(answer)

while True:
    question = input("What would you like to know about the event (type 'exit' to quit): ")
    
    if question.lower() == "exit":
        print("Exiting... Goodbye!")
        break  # Exit the loop
    
    answer = get_answer_from_event(question)
    print("ANSWER:", answer)
